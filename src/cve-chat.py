from embeddings import load_model as embed_model, vec_search
from llm import load_model as llm_model, chat, message
from db import fetch_cves_by_ids

DB_PATH = "data/db/docstore.db"
LLM_PATH = "models/llm/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"

EMBED = embed_model()  # returns fastembed TextEmbedding(...)
LLM = llm_model(LLM_PATH, False)  # returns llama_cpp.Llama


def answer(query: str, k: int = 5, max_items: int = 8) -> str:
    # vec retrieval (add your tf-idf fusion later)
    vres = vec_search(EMBED, DB_PATH, query)
    top_ids = [cid for cid, _ in vres[:k]]

    # fetch rows (preserve order)
    df = fetch_cves_by_ids(DB_PATH, top_ids)
    msgs = message(query, df, max_items=max_items)

    print(msgs)

    resp = chat(LLM, msgs)
    return resp["choices"][0]["message"]["content"]


def main():

    import argparse

    ap = argparse.ArgumentParser()
    ap.add_argument("--query")
    args = ap.parse_args()

    db = "data/db/docstore.db"
    llm = "models/llm/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"

    print("CVE chat â€” type 'exit' to quit.")
    while True:
        try:
            q = input("> ").strip()
        except (EOFError, KeyboardInterrupt):
            print()
            break
        if not q or q.lower() in {"exit", "quit"}:
            break
        print(answer(q))
        print("-" * 72)


if __name__ == "__main__":
    main()
